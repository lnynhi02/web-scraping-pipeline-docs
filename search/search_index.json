{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DOCUMENTATION","text":""},{"location":"#overview","title":"Overview","text":"<p>This pipeline is designed to automate the extraction, processing, and storage of job postings from the TOPCV website. It ensures data is cleaned, transformed, and structured for efficient querying and analysis. The processed data can be further utilized for visualization and reporting.</p> <p>For a step-by-step deployment guide, visit the Deployment. Happy coding!</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<ol> <li> <p>Data Ingestion: The pipeline collects job postings from the TOPCV website, extracting relevant information for further processing.</p> </li> <li> <p>Data Processing: Extracted data is cleaned, transformed, and formatted to ensure consistency before being stored in a PostgreSQL database.</p> </li> <li> <p>Workflow Orchestration: Airflow manages and schedules the entire workflow, automating data extraction, processing, and storage while maintaining task dependencies and execution order.</p> </li> </ol>"},{"location":"#technical-notes","title":"Technical Notes","text":""},{"location":"#1-error-handling","title":"1. Error Handling","text":"<p>To ensure data integrity and prevent unnecessary execution of downstream tasks, the pipeline includes error-handling mechanisms.</p> <p>SQL File Validation: Before executing database operations, the <code>check_sql_file</code> task verifies whether the <code>postgres_query.sql</code> file contains any <code>INSERT</code> statements.</p> <p>If the file is empty, the pipeline raises an <code>AirflowSkipException</code>, preventing unnecessary execution.</p> <pre><code>def check_sql_file(**kwargs):\n    postgres_sql_file = os.path.join(os.path.dirname(__file__), '..', 'tmp', 'postgres_query.sql')\n\n    if os.path.exists(postgres_sql_file) and os.path.getsize(postgres_sql_file) &gt; 0:\n        logging.info(\"SQL file is not empty. Proceeding with execution.\")\n    else:\n        logging.info(\"No SQL queries to execute.\")\n        raise AirflowSkipException(\"Skipping task because SQL file is empty\")\n</code></pre>"},{"location":"#2-web-scraping-with-playwright","title":"2. Web Scraping with Playwright","text":"<p>Initially, BeautifulSoup was considered for web scraping; however, due to website restrictions, Playwright was used instead.</p> <ul> <li>Headless Mode Issues: Running Playwright in <code>headless=True</code> mode caused failures, so <code>headless=False</code> was used.</li> <li>Virtual Display Setup: To avoid UI rendering issues in a containerized environment, <code>pyvirtualdisplay</code> is installed.</li> </ul>"},{"location":"#3-data-processing-storage","title":"3. Data Processing &amp; Storage","text":"<p>The pipeline follows a structured data processing workflow:</p> <ul> <li>Staging Table: Data is first written to a staging table before transformation.</li> <li>Cleaning &amp; Transformation: Data is cleaned and formatted using <code>DictCursor</code> to access table columns as key-value pairs. Using <code>DictCursor</code> enables direct access to values, for example by calling <code>job['title']</code>, which enhances code readability and maintainability.</li> <li>Incremental Processing: Only new job postings (<code>posted_date &gt; last_processed_time</code>) are processed to avoid duplication.</li> </ul> <pre><code>def clean_data(**kwargs):\n    conn = get_connection()\n    cur = conn.cursor(cursor_factory=DictCursor)\n\n    query = \"SELECT * FROM staging_table\"\n    last_processed_time = read_last_processed_time()\n\n    if last_processed_time:\n        query += \" WHERE posted_date &gt; %s\"\n        cur.execute(query, (last_processed_time,))\n    else:\n        cur.execute(query)\n\n    scraped_jobs = cur.fetchall()\n    cleaned_jobs = []\n    for job in scraped_jobs:\n        cleaned_jobs.append({\n            'title': clean_title(job['job_name']),\n            'salary': clean_salary(job['salary']),\n            'company': job['job_company'],\n            'update': pendulum.instance(job['posted_date']).in_timezone('Asia/Ho_Chi_Minh'),\n        })\n\n    kwargs['ti'].xcom_push(key='cleaned_data', value=cleaned_jobs)\n</code></pre>"},{"location":"#4-timezone-management-with-pendulum","title":"4. Timezone Management with Pendulum","text":"<p>Airflow runs all timestamps in UTC by default, which can cause inconsistencies. To avoid this, the pendulum library ensures all timestamps are converted to <code>Asia/Ho_Chi_Minh</code>(my local timezone) before pushing data to XCom.</p> <pre><code>transformed_jobs.append({\n    'update': pendulum.instance(job['posted_date']).in_timezone('Asia/Ho_Chi_Minh'),\n    'due_date': pendulum.instance(job['due_date']).in_timezone('Asia/Ho_Chi_Minh')\n})\n</code></pre>"},{"location":"#5-data-transfer-using-xcom","title":"5. Data Transfer Using XCom","text":"<p>To maintain a structured data flow, Airflow\u2019s XCom is used for inter-task communication.</p> <ul> <li>The <code>clean_data</code> task pushes processed job postings to XCom.</li> <li>The <code>transform_data</code> task pulls this data for further processing.</li> <li>Finally, <code>write_sql_query</code> generates SQL commands based on transformed data.</li> </ul>"},{"location":"#future-enhancements-limitations","title":"Future Enhancements &amp; Limitations","text":""},{"location":"#limitations","title":"Limitations","text":""},{"location":"#xcom-data-size-limit","title":"XCom Data Size Limit","text":"<p>Airflow's XCom is designed for small data exchanges between tasks.</p> <ul> <li>Storing large datasets (e.g., full job listings) might have a memory overflow error.</li> <li>Airflow XComs have a limited size and that depeonds on the database we use:<ul> <li>The default size limit for an XCom value stored in the Airflow metadata database is 48KB.  </li> <li>SQLite: 2GB</li> <li>Postgres: 1GB</li> <li>MySQL: 64KB</li> </ul> </li> </ul> <p>Ref: https://marclamberti.com/blog/airflow-xcom/#XCom_limitations</p>"},{"location":"#future-enhancements","title":"Future Enhancements","text":""},{"location":"#1-automated-reporting-visualization","title":"1. Automated Reporting &amp; Visualization","text":"<p>After storing job data, the next step could be to generate reports or integrate with BI tools.  </p> <p>\ud83d\udd39 Possible options: - Exporting reports to Google Sheets, Excel, or PDFs. - Connecting PostgreSQL with Power BI, Metabase, or Superset for visualization. </p>"},{"location":"#2-stream-processing-for-real-time-job-scraping","title":"2. Stream Processing for Real-Time Job Scraping","text":"<p>Currently, the pipeline operates in batch mode, meaning job postings are scraped and processed at scheduled intervals. This may cause delays in updating the latest job listings.  </p> <p>\ud83d\udd39 Enhancement: Introduce stream processing to handle job postings in real time.  </p> <p>Approach - Continuously monitor job listings for updates. - Trigger data ingestion as soon as a new job posting appears. - Store real-time data in a temporary location (e.g., Kafka, Redis, or a staging table).  </p> <p>Hybrid Processing - Streaming Mode: Ingest and process new postings in real-time. - Batch Mode: Run scheduled jobs for historical data analysis and reporting.  </p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>Although this pipeline still has limitations and room for improvement, it has been a great learning experience. It pushed me to think about error handling, data flow between tasks, and different ways to utilize Airflow more effectively. Most importantly, this pipeline helped me develop problem-solving skills\u2014thinking critically and exploring alternative approaches when tackling challenges.</p> <p>There is no best\u2014only better, and the same applies to this pipeline. Every challenge has been a step toward improvement, and each iteration brings new insights, reinforcing the mindset that continuous learning and refinement are key to building robust data workflows.s</p>"},{"location":"deployment/","title":"\ud83d\udcbc JobScraper: Automated Job Data Pipeline","text":""},{"location":"deployment/#overview","title":"Overview","text":"<p>JobScraper is a web scraping and data pipeline project designed to automatically extract job postings from the TOPCV website (a website that posts job listings in Vietnam). The extracted data is cleaned, transformed, and stored in a structured database for easy querying and analysis. This project provides insights into the current job market and can be extended for data visualization and reporting purposes.</p> <p></p>"},{"location":"deployment/#achievements","title":"\ud83c\udfc6 Achievements","text":"<ul> <li>Stored new jobs daily in the PostgreSQL database to keep an updated repository.</li> <li>Cleaned the 'job_name' and 'salary' columns to enhance query performance and ensure accurate data retrieval.</li> <li>Created a 'due_date' column to facilitate easy tracking of job deadlines.</li> <li>Utilized a stored procedure to update the 'deadline' column daily, for example, changing 'C\u00f2n 24 ng\u00e0y \u0111\u1ec3 \u1ee9ng tuy\u1ec3n' to 'C\u00f2n 23 ng\u00e0y \u0111\u1ec3 \u1ee9ng tuy\u1ec3n' the following day.</li> </ul>"},{"location":"deployment/#table-of-contents","title":"\ud83d\udcd5 Table Of Contents","text":"<ul> <li>\u2699\ufe0f Local Setup</li> <li>\ud83d\udcbb Deployment<ul> <li>\ud83d\udee2\ufe0f Postgres Setup</li> <li>\ud83d\ude80 Airflow Setup</li> <li>\ud83d\udcdc SQL Query</li> <li>Technical Documentation</li> </ul> </li> </ul>"},{"location":"deployment/#local-setup","title":"\u2699\ufe0f Local Setup","text":""},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Docker for running Airflow.</li> <li>Install Python.</li> <li>Install PostgreSQL.</li> </ul> <p>Clone, fork, or download this GitHub repository on your local machine using the following command:</p> <pre><code>git clone https://github.com/lnynhi02/web-scraping-etl-pipeline.git\n</code></pre>"},{"location":"deployment/#project-structure","title":"Project Structure","text":"<pre><code>web-scraping-etl-pipeline/\n\u251c\u2500\u2500 airflow/\n\u2502   \u251c\u2500\u2500 dags/\n\u2502   \u2502   \u2514\u2500\u2500 topcv_flow.py\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 config.ini\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 create_table.py\n\u2502   \u251c\u2500\u2500 topcv_pipeline.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 tmp/\n\u2502   \u251c\u2500\u2500 last_processed_time.json\n\u2502   \u2514\u2500\u2500 postgres_query.sql\n\u251c\u2500\u2500 .env\n\u251c\u2500\u2500 docker-compose.yaml\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"deployment/#directory-breakdown","title":"Directory Breakdown","text":"<ul> <li> <p><code>airflow/</code>: Contains <code>dags/topcv_flow.py</code>, which holds the DAG responsible for orchestrating the TopCV-related workflow. The Dockerfile builds a custom image based on <code>apache/airflow:2.8.0</code>, setting up the Airflow environment and installing Python dependencies. It also installs necessary system libraries, along with tools like <code>pyvirtualdisplay</code>, <code>playwright</code>, and <code>Chromium</code> for browser-based tasks. This setup is tailored for running Airflow workflows that involve web scraping or automated browser interactions.</p> </li> <li> <p><code>config/</code>: Contains <code>config.ini</code>, which includes the configuration for your PostgreSQL database. Please change the values of the <code>database</code> and <code>password</code> to your own credentials.</p> </li> <li> <p><code>pipelines/</code>: Contains all the core tasks of the pipeline:</p> <ul> <li><code>create_table.py</code>: Script for creating PostgreSQL tables.</li> <li><code>utils.py</code>: Functions such as <code>clean_title</code>, <code>clean_salary</code>, <code>transform_salary</code>, and <code>calculate_dates</code> for data cleaning and transformation.</li> <li><code>topcv_pipeline.py</code>: Uses functions from <code>utils.py</code> to clean and process job data scraped from the web.</li> </ul> </li> <li> <p><code>tmp/</code>:</p> <ul> <li><code>last_processed_time.json</code>: Stores the timestamp of the last processed job.</li> <li><code>postgres_query.sql</code>: Holds the SQL INSERT statements for each job. These files help track processed jobs and insert scraped data into PostgreSQL.</li> </ul> </li> <li> <p><code>docker-compose.yaml</code>: Configures Airflow services.</p> </li> </ul> <p>To set up the local development environment, begin by creating a virtual environment and installing <code>psycopg2-binary</code>. This package is only needed to run <code>create_table.py</code> locally, so there's no need to install all the packages listed in <code>requirements.txt</code> at this point. The packages from <code>requirements.txt</code> will be installed later in the Docker environment via the <code>Dockerfile</code>.</p> <ol> <li>Create a virtual environment:    <pre><code>python -m venv venv\n</code></pre></li> <li>Activate the virtual environment:</li> <li>Windows PowerShell: <pre><code>venv\\Scripts\\Activate\n</code></pre></li> <li>Linux/macOS: <pre><code>source venv/bin/activate\n</code></pre></li> <li>Install the necessary package:    <pre><code>pip install psycopg2-binary==2.9.9\n</code></pre></li> </ol>"},{"location":"deployment/#deployment","title":"\ud83d\udcbb Deployment","text":""},{"location":"deployment/#postgres-setup","title":"<code>Postgres Setup</code>","text":"<p>Before setting-up our airflow configurations, let\u2019s create the Postgres database that will persist our data. I prefer using the pgAdmin 4 tool for this, however any other Postgres development platform can do the job.</p> <p>When installing postgres, you need to setup a password that we will need later to connect to the database from the Spark environment. You must remember the password to reconnect to the database servers. You can also leave the port at 5432. If your installation has succeeded, you can start pgadmin and you should observe something like this window:</p> <p></p> <p>Since we have many columns for the table we want to create, we opted to use a script with psycopg2, a PostgreSQL database adapter for Python, to create the table and add its columns. And we have installed the psycopg2-binary package in the <code>requirements.txt</code>.</p> <p>You can run the Python script with the following command: <pre><code>python pipelines/create_table.py\n</code></pre> I use <code>config.ini</code> to access the database configurations, allowing you to modify the application settings easily. Alternatively, if you prefer to use a different method, you can make slight adjustments to the script accordingly. The <code>config.ini</code> file looks as follows: <pre><code>[database]\nhost=host.docker.internal\nport=5432\nuser=postgres\npassword=change_me\ndatabase=change_me\n</code></pre></p>"},{"location":"deployment/#airflow-setup","title":"<code>Airflow Setup</code>","text":"<p>Let\u2019s take a look at the Directed Acyclic Graph (DAG) that will outline the sequence and dependencies of tasks, enabling Airflow to manage their execution. <pre><code>from airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow import DAG\nimport airflow.utils.dates\nimport logging\nimport sys\nimport os\n\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'pipelines'))\nfrom topcv_pipeline import scrape_data, clean_data, transform_data, write_sql_query, check_sql_file\n\nlogging.basicConfig(level=logging.INFO)\nTEMPLATE_SEARCH_PATH = os.path.join(os.path.dirname(__file__), '..', 'tmp')\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': airflow.utils.dates.days_ago(1)\n}\n\nwith DAG(\n    'job_scraper',\n    default_args=default_args,\n    template_searchpath=TEMPLATE_SEARCH_PATH,\n    schedule_interval='@daily',\n    catchup=False\n) as dag:\n    scrape_data_task = PythonOperator(\n        task_id='scrape_data_task',\n        python_callable=scrape_data,\n        provide_context=True,\n        op_kwargs={'url': 'https://www.topcv.vn/viec-lam-it'},\n    )\n\n    clean_data_task = PythonOperator(\n        task_id='clean_data_task',\n        python_callable=clean_data,\n        provide_context=True\n    )\n\n    transform_data_task = PythonOperator(\n        task_id='transform_data_task',\n        python_callable=transform_data,\n        provide_context=True\n    )\n\n    write_sql_query_task = PythonOperator(\n        task_id='write_sql_query_task',\n        python_callable=write_sql_query,\n        provide_context=True\n    )\n\n    check_sql_file_task = PythonOperator(\n        task_id='check_sql_file_task',\n        python_callable=check_sql_file,\n        provide_context=True\n    )\n\n    write_to_postgres_task = PostgresOperator(\n        task_id='write_to_postgres_task',\n        postgres_conn_id='postgres_conn',\n        sql='postgres_query.sql',\n        trigger_rule='all_success'\n    )\n\nscrape_data_task &gt;&gt; clean_data_task &gt;&gt; transform_data_task &gt;&gt; write_sql_query_task &gt;&gt; check_sql_file_task &gt;&gt; write_to_postgres_task\n</code></pre></p> <ul> <li>The dag includes all the tasks that are imported from the <code>topcv_pipeline.py</code></li> <li>The tasks are set to execute daily.</li> <li>The first task is the Scrape Data Task. This task scrapes data from the TOPCV website into a staging table in Postgres database, initiating the data processing workflow.</li> <li>The second task, Clean Data Task, will retrieve new, unprocessed jobs from the staging table, clean the 'title' and 'salary' fields by using <code>clean_title()</code> and <code>clean_salary()</code> function from <code>utils.py</code>, and then push the cleaned data into XCom for later transformation.</li> <li>The third task is the Transform Data Task, which pulls cleaned data from XCom, uses the <code>transform_salary()</code> from <code>utils.py</code> to calculate the average salary, and then pushes the results back to XCom.</li> <li>The fourth task, Write SQL Query Task, pulls transformed data from XCom and then generates INSERT SQL commands for each job, saving them to <code>postgres_query.sql</code> for use with the <code>PostgresOperator</code> in downstream task.</li> <li>The fifth task, Check SQL File Task, checks whether the <code>postgres_query.sql</code> file contains any SQL commands. If it does, the downstream tasks will be executed; if the file is empty, the downstream tasks will be skipped.</li> <li>The final task is the Write To Postgres Task. It uses the PostgresOperator for execution, running the SQL commands from the <code>postgres_query.sql</code> file and storing the jobs in the PostgreSQL database.</li> </ul> <p>Now, we just need to run Airflow in Docker. However, we need to create some environment variables that will be used by docker-compose.</p>"},{"location":"deployment/#linux","title":"Linux:","text":"<pre><code>echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env\n</code></pre>"},{"location":"deployment/#windows","title":"Windows:","text":"<p>Find the UID by running: <pre><code>whoami /user\n</code></pre> Take the 4 numbers at the end and run: <pre><code>Set-Content -Path .env -Value \"AIRFLOW_UID=xxxx\"\n</code></pre></p>"},{"location":"deployment/#create-airflowlogs-folder","title":"Create <code>airflow/logs/</code> folder:","text":"<ul> <li>Windows (PowerShell): <pre><code>New-Item -ItemType Directory -Path \"airflow/logs\"\n</code></pre></li> <li>Linux: <pre><code>mkdir -p airflow/logs\n</code></pre></li> </ul>"},{"location":"deployment/#start-airflow","title":"Start Airflow:","text":"<pre><code>docker network create airflow\ndocker-compose up -d\n</code></pre> <p>Now we can access the Airflow UI at <code>localhost:8080</code>. Use the username <code>airflow</code> and the password <code>airflow</code> to log in.</p> <p></p> <ul> <li>We can see the dag <code>job_scraper</code>. </li> </ul> <p></p> <ul> <li> <p>Before running the DAG, we need to establish a Postgres Connection in Airflow to enable connectivity with the Postgres database. This connection is essential for executing the <code>write_to_postgres_task</code> using the <code>PostgresOperator</code>.</p> </li> <li> <p>Navigate to the Admin section at the top and select Connections. Then, click on the + icon to add a new connection.</p> </li> </ul> <p></p> <ul> <li>Since we are connecting from Airflow running in a Docker container, set the host to host.docker.internal. Ensure you also enter your database name and password accordingly. The Connection ID will later be utilized in the postgres_conn_id parameter within the <code>write_to_postgres_task</code>.</li> </ul> <p></p> <ul> <li>Let's go and run the DAG.</li> </ul> <p></p> <ul> <li>You can monitor the log for each task to gain insights into the scraping process and see how many jobs have been collected.</li> </ul> <p></p> <p></p>"},{"location":"deployment/#sql-query","title":"<code>SQL Query</code>","text":"<p>Once all the data is loaded into the database, we need to perform some cleaning. <pre><code>UPDATE jobs_table\nSET salary = NULL\nWHERE salary = 'Th\u1ecfa thu\u1eadn';\n\nALTER TABLE jobs_table\nALTER COLUMN salary TYPE numeric USING salary::numeric;\n\nALTER TABLE jobs_table\nALTER COLUMN salary TYPE integer USING ROUND(salary);\n</code></pre></p> <p>We need to create a stored procedure to update the remaining time for jobs that are still open for applications. <pre><code>CREATE OR REPLACE PROCEDURE update_deadline()\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    job_record RECORD;\n    time_remaining INTERVAL;\nBEGIN\n    FOR job_record IN SELECT * FROM jobs_table LOOP\n        time_remaining = jobs_table.due_date - CURRENT_TIMESTAMP;\n\n        IF time_remaining &gt; INTERVAL '0 seconds' THEN \n            IF time_remaining &lt; INTERVAL '1 minutes' THEN\n                UPDATE jobs_table\n                SET remaining_time = 'C\u00f2n ' || EXTRACT(SECOND FROM time_remaining) || ' gi\u00e2y \u0111\u1ec3 \u1ee9ng tuy\u1ec3n'\n                WHERE job_link = job_record.job_link;\n            ELSIF time_remaining &lt; INTERVAL '1 hour' THEN\n                UPDATE jobs_table\n                SET remaining_time = 'C\u00f2n ' || EXTRACT(MINUTE FROM time_remaining) || ' ph\u00fat \u0111\u1ec3 \u1ee9ng tuy\u1ec3n'\n                WHERE job_link = job_record.job_link;\n            ELSIF time_remaining &lt; INTERVAL '1 day' THEN \n                UPDATE jobs_table\n                SET remaining_time = 'C\u00f2n ' || EXTRACT(HOUR FROM time_remaining) || ' gi\u1edd \u0111\u1ec3 \u1ee9ng tuy\u1ec3n'\n                WHERE job_link = job_record.job_link;\n            ELSE\n                UPDATE jobs_table\n                SET remaining_time = 'C\u00f2n ' || EXTRACT(DAY FROM time_remaining) || ' ng\u00e0y \u0111\u1ec3 \u1ee9ng tuy\u1ec3n'\n                WHERE job_link = job_record.job_link;\n            END IF;\n        ELSE\n            UPDATE jobs_table\n            SET remaining_time = '\u0110\u00e3 h\u1ebft th\u1eddi gian \u1ee9ng tuy\u1ec3n'\n            WHERE job_link = job_record.job_link;\n        END IF;\n    END LOOP;\nEND;\n$$;\n</code></pre></p>"},{"location":"deployment/#querying-data","title":"Querying Data","text":"<pre><code>-- Get a list of jobs with application deadline within the next 10 days\nSELECT job_name, job_link, salary, job_location, remaining_time, due_date\nFROM jobs_table\nWHERE due_date &lt;= NOW() + INTERVAL '20 DAYS';\n\n-- Find jobs with salary greater than 15 million VND\nSELECT job_name, job_link, company_name, salary, job_location\nFROM jobs_table\nWHERE salary &gt; 15;\n\n-- Get a list of jobs in order of the most recently posted\nSELECT job_name, company_name, posted_date\nFROM jobs_table\nORDER BY posted_date DESC;\n\n-- Get the total number of jobs available in Ho Chi Minh City\nSELECT COUNT(*)\nFROM jobs_table\nWHERE job_location LIKE '%H\u1ed3 Ch\u00ed Minh%';\n\n-- Find the top 10 highest-paying jobs in Ho Chi Minh City\nSELECT job_name, job_link, company_name, salary\nFROM jobs_table\nWHERE job_location LIKE '%H\u1ed3 Ch\u00ed Minh%'\nORDER BY salary DESC\nLIMIT 10;\n</code></pre>"},{"location":"deployment/#technical-documentation","title":"Technical Documentation","text":""}]}